\section{Introduction}\label{sec:intro}

%Global value numbering is an analysis performed by many highly-optimizing
%compilers.  Its roots run deep, and there have been many developments along the
%way, both theoretical and practical.  Using the results of this analysis, the
%compiler can identify expressions in the source code that produce the same
%value---not just by lexical comparison, but by proving equivalences between
%what's actually computed by the code.  These expressions can then be simplified
%by further algorithms for redundancy elimination.  This is the very essence of
%most compiler optimizations: avoid redundant computation, giving us code that
%runs as quickly as possible while still following what the programmer
%originally wrote.  Of course, we want this all to be done quickly by the
%compiler, so we're still concerned about the efficiency of the optimizations
%themselves.
%
%High-level, dynamic languages tend to suffer from efficiency issues: they're
%often interpreted rather than compiled, and perform no heavy optimization of
%the source code.  However, the Factor language fills an intriguing design
%niche, as it's very high-level yet still fully compiled.  It's still young,
%though, so its compiler craves all the improvements it can get---among them, a
%global value numbering (and subsequent redundancy elimination) pass.
%
%The bulk of this proposal focuses on the history of value numbering as told
%through the volume of work done on designing and implementing various
%algorithms.  Exploring these gives us a solid understanding of the work that
%needs to be done, what methods currently exist, and the considerations that go
%into writing a global value numbering pass for a compiler.  After this, we turn
%to the main deliverable of the thesis, which is the implementation of such
%algorithms in Factor.  The proposed research will thus produce data about
%performing aggressive, state-of-the-art optimizations in a high-level language.
%
%\section{Literature Survey}
%
%\subsection{Preliminaries}
%
%Compilers translate programs written in a source language (e.g., Java) into
%semantically equivalent programs in some target language (e.g., assembly code).
%They let us make our source language arbitrarily abstract so we can
%\emph{write} programs in ways that humans can understand while letting the
%computer \emph{execute} programs in ways that machines can understand.  In a
%perfect world, such translation would be straightforward.  In
%%this miserable existence we call ``
%reality%
%%''
%, straightforward compilation results in clunky target code that performs a lot
%of redundant computations.  We certainly want programs to be fast---not just
%the compiled result, but also the compiler itself.  A compiler thus has three
%duties: preserve semantics, produce efficient code, and do it quickly.
%
%To this end, typical compilers go through a stage of \defn{optimization},
%whereby a number of semantics-preserving transformations are applied to an
%\defn{intermediate representation} of the source code that (hopefully) produce
%a more efficient version.  Optimizers tend to work in \defn{phases}, applying
%specific transformations during any given phase.  The order of these phases is
%important, since one transformation may open up opportunities for another.  The
%most common intermediate representation (or at least the one we'll use here) is
%the \defn{control-flow graph} (or \defn{CFG}, not to be confused with the
%abbreviation for ``context-free grammar'').  
%
%\begin{figure}
%  \begin{center}
%    \begin{minipage}{0.3\linewidth}
%      \begin{lstlisting}
%    i := 0
%    j := 0
%L1: i := i + 1
%    j := j + 1
%    if $P$ goto L1
%      \end{lstlisting}
%    \end{minipage}
%    \vrule
%    \begin{minipage}{0.3\linewidth}
%      \begin{tikzpicture}[node distance=.85in,>=latex]
%        \node[draw,label=right:$B_1$] (0) at (0,-1.25) {
%          \begin{lstlisting}
%i := 0
%j := 0
%          \end{lstlisting}
%        };
%        \node[draw,label=right:$B_2$] (1) [below of=0] {
%          \begin{lstlisting}
%i := i + 1
%j := j + 1
%if $P$ goto $B_2$
%          \end{lstlisting}
%        };
%
%        \draw[->] (0,0) -- (0);
%        \draw[->] (0) -- (1);
%        \draw (1.south)
%              edge[->,out=180,in=270,controls=+(35:-4) and +(-35:-4)]
%              (1.north);
%      \end{tikzpicture}
%    \end{minipage}
%  \end{center}
%  \caption{CFG Construction}
%  \label{fig:cfg-construction}
%\end{figure}
%
%For the purposes of this survey, we'll assume the compiler is transforming a
%simple three-address language, so-called because it's generally composed of
%instructions of the form \lstinline|x := y $op$ z|.  The semantics aren't
%terribly important, but the code should be clear to anyone familiar with basic
%assembly, as the language is modeled after common RISC machine code.  CFGs are
%arrangements of instructions into \defn{basic blocks}: maximal sequences of
%``straight-line'' code, where control does not transfer out of or into the
%middle of the block (e.g., by a \lstinline|goto|).  Then, directed edges are
%added between blocks to represent control flow---either from a \lstinline|goto|
%to its target, or from the end of a basic block to the start of the next one
%\cite{DragonBook}.  See Figure~\vref{fig:cfg-construction}.
%
%Probably the most popular intermediate representation in modern compilers is a
%variation of the CFG called \defn{static single assignment} (or \defn{SSA})
%form, wherein every variable in the program is defined by exactly one
%statement.  This simplifies the properties of variables, which helps
%optimizations perform faster and with better results.  The optimizations in
%this thesis will operate on SSA form.
%
%\begin{figure}
%\begin{center}
%  \begin{minipage}{0.4\linewidth}
%    \begin{tikzpicture}[node distance=1.0in,>=latex]
%    \node[draw,label=right:$B_1$] (1) at (0,-1.25)
%      {\lstinline|if $P$ goto $B_3$|};
%    \node[draw,label=left:$B_2$] (2) [below left of=1]
%      {\lstinline|x := 5|};
%    \node[draw,label=right:$B_3$] (3) [below right of=1]
%      {\lstinline|x := 10|};
%    \node[draw,label=right:$B_4$] (4) [below right of=2]
%      {\lstinline|y := x + 1|};
%
%    \draw[->] (0,0) -- (1);
%    \draw (1) edge[->] (2)
%              edge[->] (3)
%          (2) edge[->] (4)
%          (3) edge[->] (4);
%    \end{tikzpicture}
%  \end{minipage}
%  \vrule
%  \begin{minipage}{0.4\linewidth}
%    \begin{tikzpicture}[node distance=1.0in,>=latex]
%    \node[draw,label=right:$B_1$] (1) at (0,-1.25)
%      {\lstinline|if $P$ goto $B_3$|};
%    \node[draw,label=left:$B_2$] (2) [below left of=1]
%      {\lstinline|x$_0$ := 5|};
%    \node[draw,label=right:$B_3$] (3) [below right of=1]
%      {\lstinline|x$_1$ := 10|};
%    \node[draw,label=right:$B_4$] (4) [below right of=2]
%      {\lstinline|y$_{~}$ := x$_?$ + 1|};
%
%    \draw[->] (0,0) -- (1);
%    \draw (1) edge[->] (2)
%              edge[->] (3)
%          (2) edge[->] (4)
%          (3) edge[->] (4);
%    \end{tikzpicture}
%  \end{minipage}
%\end{center}
%\caption{SSA Construction Ambiguity}
%\label{fig:ssa-construction}
%\end{figure}
%
%At a high level, SSA form is constructed from a non-SSA CFG by giving unique
%names to the targets of each assignment (thus guaranteeing the SSA property),
%and by replacing uses of the original assignment with this new name.  But
%control flow can produce ambiguity.  In Figure~\vref{fig:ssa-construction}, the
%CFG on the left is being transformed into SSA form.  The original has two
%definitions of \lstinline|x| (in $B_2$ and $B_3$), either of which may reach
%the use of \lstinline|x| in $B_4$.  In SSA form, we give unique names to these
%two \lstinline|x| assignments, so which ``version'' do we use in $B_4$:
%\lstinline|x$_0$| or \lstinline|x$_1$|?
%
%\begin{figure}
%\begin{center}
%\begin{tikzpicture}[node distance=1.0in,>=latex]
%\node[draw,label=right:$B_1$] (1) at (0,-1.25) {\lstinline|if $P$ goto $B_3$|};
%\node[draw,label=left:$B_2$] (2) [below left of=1] {\lstinline|x$_0$ := 5|};
%\node[draw,label=right:$B_3$] (3) [below right of=1] {\lstinline|x$_1$ := 10|};
%\node[draw,label=right:$B_4$] (4) [below right of=2] {
%\begin{lstlisting}
%x$_2$ := $\phi$(x$_0$,x$_1$)
%y$_{~}$ := x$_2$ + 1
%\end{lstlisting}
%};
%
%\draw[->] (0,0) -- (1);
%\draw (1) edge[->] (2)
%          edge[->] (3)
%      (2) edge[->] (4)
%      (3) edge[->] (4);
%\end{tikzpicture}
%\end{center}
%\caption{$\phi$ Insertion}
%\label{fig:ssa-phi}
%\end{figure}
%
%To remedy this problem, SSA introduces a ``phony function'', $\phi$.  It's
%applied to two versions of variables that may reach the point where it's
%inserted.  While $\phi$ doesn't perform any literal computation, conceptually
%it selects the ``correct'' operand, depending on the control flow.  In
%Figure~\vref{fig:ssa-phi}, we correct the CFG of
%Figure~\ref{fig:ssa-construction} by inserting a new variable that's assigned
%to the result of the $\phi$ function and replacing the use of \lstinline|x|
%with this new variable.  Thus, if control flow goes $B_1 \to B_2 \to B_4$,
%$\phi$ selects \lstinline|x$_0$|.  If control flow goes $B_1 \to B_3 \to B_4$,
%$\phi$ selects \lstinline|x$_1$|.
%
%In principle, we could place any number of $\phi$-functions anywhere in the
%program (e.g., trivial ones like \lstinline|y := $\phi($x,x$)$|).  In practice,
%we insert them in as few places as possible---only at the beginnings of blocks
%where we need to.  There are many algorithms for efficient SSA construction
%with proper $\phi$ insertion (e.g., see \citeasnoun{SSAConstruction}) which are
%beyond the scope of this thesis.  For the rest of this proposal, it's assumed
%that CFGs are in SSA form.
