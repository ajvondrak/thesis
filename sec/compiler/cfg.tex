\subsection{Low-level Optimizations}\label{sec:compiler:cfg}

The low-level \gls{IR} in \factor|compiler.cfg| takes the more conventional
form of a \gls{CFG}.  A \gls{CFG} (not to be confused with ``context-free
grammar'') is an arrangement of instructions into \term{basic blocks}: maximal
sequences of ``straight-line'' code, where control does not transfer out of or
into the middle of the block.  Directed edges are added between blocks to
represent control flow---either from a branching instruction to its target, or
from the end of a basic block to the start of the next one\todo{cite}.
Construction of the low-level \gls{IR} proceeds by analyzing the control flow
of the high-level \gls{IR} and converting the nodes of \cref{sec:compiler:tree}
into lower-level, more conventional instructions modeled after typical assembly
code.  There are over a hundred of these instructions, but many are simply
different versions of the same operation.  For instance, while instructions are
generally called on \term{virtual registers} (represented in Factor simply by
integers), there are \term{immediate} versions of instructions.  The
\factor|##add| instruction, as an example, represents the sum of the contents
of two registers, but \factor|##add-imm| sums the contents of one register and
an integer literal.  Other instructions are inserted to make stack reads and
writes explicit, as well as to balance the height.  Below is a categorized list
of all the instruction objects (each one is a subclass of the \factor|insn|
tuple).

\todo[inline]{Is the complete list really necessary?}
\input{sec/compiler/insns}

\inputlst{optimize-cfg}

\inputfig{optimize-tail-calls}

By translating the high-level \gls{IR} into instructions that manipulate
registers directly, we reveal further redundancies that can be optimized away.
The \factor|optimize-cfg| word in \vref{lst:optimize-cfg} shows the passes
performed in doing this.  The first word, \factor|optimize-tail-calls|,
performs tail call elimination on the \gls{CFG}.
%
\term{Tail calls}~\todo{used in \cref{sec:compiler:tree}, not defined} are those
that occur within a procedure and whose results are immediately returned by
that procedure.  Instead of allocating a new call stack frame, we may convert
tail calls into simple jumps, since afterwards the current procedure's call
frame isn't really needed.  In the case of recursive tail calls, we can convert
special cases of recursion into loops in the \gls{CFG}, so that we won't
trigger call stack overflows.  For instance, consider
\vref{fig:optimize-tail-calls}, which shows the effect of
\factor|optimize-tail-calls| on the following definition:
%
\begin{center}
%
  \factor|: tail-call ( -- ) tail-call ;|
%
\end{center}
%
\noindent Note the recursive call (trivially) occurs at the end of the
definition, just before the return point.  When translated to a \gls{CFG}, this
is a \factor|##call| instruction, as seen in block $4$ to the left of
\vref{fig:optimize-tail-calls}.  This is also just before the final
\factor|##epilogue| and \factor|##return| instructions in block $8$, as blocks
$5$--$7$ are effectively empty (these excessive \factor|##branch|es will be
eliminated in a later pass).  Because of this, rather than make a whole new
subroutine call, we can convert it into a \factor|##branch| back to the
beginning of the word, as in the \gls{CFG} to the right.

\inputfig{delete-useless-conditionals}

The next pass in \vref{lst:optimize-cfg} is
\factor|delete-useless-conditionals|, which removes branches that go to the
same basic block.  This situation might occur as a result of optimizations
performed in the high-level \gls{IR}.  To see it in action,
\vref{fig:delete-useless-conditionals} shows the transformation on a
purposefully useless conditional,
%
\factor|[ ] [ ] if|.
%
Before removing the useless conditional, the \gls{CFG} \factor|##peek|s at the
top of the data stack
%
(\factor|D 0|),
%
storing the result in the virtual register \factor|1|.  This value is popped,
so we decrement the stack height
%
(\factor|##inc-d -1|).
%
Then, \factor|##compare-imm-branch| in block $2$ compares the value in the
virtual register \factor|2| (which is a copy of \factor|1|, the top of the
stack) to the immediate value \factor|f| to see if it's not equal (signified by
\factor|cc/=|).  However, both branches jump through several empty blocks and
merge at the same destination.  Thus, we can remove both branches and replace
\factor|##compare-imm-branch| with an unconditional \factor|##branch| to the
eventual destination.  We see this on the right of
\vref{fig:delete-useless-conditionals}.

\inputfig{split-branches}

In order to expose more opportunities for optimization, \factor|split-branches|
will actually duplicate code.  We use the fact that code immediately following
a conditional will be executed along either branch.  If it's sufficiently
short, we copy it up into the branches individually.  That is, we change
%
\factor|[ A ] [ B ] if C|
%
into
%
\factor|[ A C ] [ B C ] if|,
%
as long as \factor|C| is small enough.  Later analyses may then, for example,
more readily eliminate one of the branches if it's never taken.
\vref{fig:split-branches} shows what such a transformation looks like on a
\gls{CFG}.  The example
%
\factor|[ 1 ] [ 2 ] if dup|
%
is essentially changed into
%
\factor|[ 1 dup ] [ 2 dup ] if|,
%
thus splitting the block with two predecessors (block $9$) on the left.

\inputfig{join-blocks}

The next pass, \factor|join-blocks|, compacts the \gls{CFG} by joining together
blocks involved in only a single control flow edge.  Mostly, this is to clean
up the myriad of empty or short blocks introduced during construction, like
sequences of a bunch of \factor|##branch|es.  \Vref{fig:join-blocks} shows this
pass on the \gls{CFG} of
%
\factor|0 100 [ 1 fixnum+fast ] times|.
%
\factor|fixnum+fast| is a specialized version of \factor|+| that suppresses
overflow and type checks.  We use it here to keep the \gls{CFG} simple.  We'll
be using this particular code to illustrate all but one of the remaining
optimization passes in \vref{lst:optimize-cfg}, as it's a motivating example
for the work in this thesis.  The passes before \factor|join-blocks| don't
change the \gls{CFG} seen on the left in \vref{fig:join-blocks}, but we get rid
of the useless \factor|##branch| blocks in the \gls{CFG} on the right.

\inputfig{normalize-height}

\Vref{fig:normalize-height} shows the result of applying
\factor|normalize-height| to the result of \factor|join-blocks|.  This phase
combines and canonicalizes the instructions that track the stack height, like
\factor|##inc-d|.  While the shuffling in this example isn't complex enough to
be interesting, neither is this phase.  It amounts to more cleanup: multiple
height changes are combined into single ones at the beginnings of the basic
blocks.  In \vref{fig:normalize-height}, this means that \factor|##inc-d| is
moved to the top of block $1$, as compared to the right of
\vref{fig:join-blocks}.

\inputfig{construct-ssa}

In converting the high-level \gls{IR} to the low-level, we actually lose the
\gls{SSA} form of \factor|compiler.tree|.  Not only does the construction do
this, but \factor|split-branches| also copies basic blocks verbatim, so any
value defined will have a duplicate definition site, violating the \gls{SSA}
property.  \factor|construct-ssa| recomputes a so-called \term{pruned}
\gls{SSA} form, wherein $\phi$ functions are inserted only if the variables are
live after the insertion point.  This cuts down on useless $\phi$ 
%
functions\todo{cite TDMSC and construction algorithm}.
%
\Vref{fig:construct-ssa} shows the reconstructed \gls{SSA} form of the
\gls{CFG} from \vref{fig:normalize-height}.

The next pass, \factor|alias-analysis|, doesn't change the \gls{CFG} of
%
\factor|0 100 [ 1 fixnum+fast ] times|,
%
so we won't have an accompanying \lcnamecref{fig:construct-ssa}.  At a high
level, \factor|alias-analysis| is easy to understand: it eliminates redundant
memory loads and stores by rewriting certain patterns of memory access.  If the
same location is loaded after being stored, we convert the latter load into a
\factor|##copy| of the value we stored.  Two reads of the same location with no
intermittent write gets the second read turned into a \factor|##copy|.
Similarly, if we see two writes without a read in the middle, the first write
can be removed.

\inputfig{value-numbering}

\factor|value-numbering| is the key focus of this thesis.  It will be detailed
in \vref{sec:vn}.  For now, it does to think of it as a combination of common
subexpression elimination and constant folding.  In \vref{fig:value-numbering}, 
we see several changes:
%
\begin{itemize}
%
  \item \factor|##load-integer 23 0| in block $1$ of \vref{fig:construct-ssa}
  (which assigns the value \factor|0| to the virtual register \factor|23|) is
  redundant, so is replaced by \factor|##copy 23 21|.
%
  \item \begin{flushleft}
  In block $2$, the last instruction
  %
  \factor|##compare-imm-branch 32 f cc/=|
  %
  is the same as
  %
  \factor|##compare-integer-branch 30 26 cc<|.
  %
  The source register (\factor|32|) of the original is a \factor|##copy| of
  \factor|31|, which itself is computed by
  %
  \factor|##compare-integer 31 30 26 cc< 9|.
  %
  So, the \factor|##compare-imm-branch| is equivalent to a simple
  \factor|##compare-integer-branch|, which doesn't use the temporary virtual
  register \factor|9| and doesn't waste time comparing against the \factor|f|
  object.
  \end{flushleft}
%
  \item The second operands in both \factor|##add|s of block $3$ are just
  constants stored by \factor|##load-integer|s.  So, these are turned into
  \factor|##add-imm|s.
\end{itemize}
%
\noindent In \cref{sec:vn}, we'll see how and why this pass fails to identify
other equivalences.

\inputfig{copy-propagation}

\inputfig{eliminate-dead-code}

\inputfig{finalize-cfg}

% dls.pdf verbatim:

%The main optimizations performed on low-level IR are local dead store and
%redundant load elimination, local value numbering, global copy propagation,
%representation selection, and instruction scheduling.  The local value
%numbering pass eliminates common subexpressions and folds expressions with
%constant operands [9].

%Following value numbering and copy propagation, a representation selection
%pass decides when to unbox floating point and SIMD values. A form of
%instruction scheduling intended to reduce register pressure is performed on
%low-level IR as the last step before leaving SSA form [39].  We use the
%second-chance binpacking variation of the linear scan register allocation
%algorithm [43, 47]. Our variant does not take $\phi$ nodes into account, so
%SSA form is destructed first by eliminating $\phi$ nodes while simultaneously
%performing copy coalescing, using the method described in [6].
